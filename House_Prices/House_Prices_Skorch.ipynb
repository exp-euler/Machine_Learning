{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1319dc6a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In the cell below we load the data in and split it into training and validating data. We also load the testing data.\n",
    "\n",
    "Furthermore, we inspect the data file and specify which columns correspond to numerical data and which ones correspond to categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9775f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('./input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing Price info, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Select categorical columns to perform Ordinal Encoding\n",
    "ord_categorical_cols = ['Street', 'Alley', 'ExterQual', 'ExterCond',\n",
    "                       'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "                       'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "                       'CentralAir', 'KitchenQual', 'Functional',\n",
    "                       'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "                       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n",
    "\n",
    "# Select categorical columns to perform OneHotEncoding\n",
    "ohe_categorical_cols = [cname for cname in X_full.columns if \n",
    "                    X_full[cname].dtype == \"object\" and\n",
    "                    cname not in ord_categorical_cols]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_full.columns if \n",
    "                X_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# For now we drop ohe columns ...\n",
    "X_full.drop(ohe_categorical_cols, axis=1, inplace=True)\n",
    "X_full.drop(ord_categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Split the data into training and validating\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59dacbe",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Transforming the data to `tensor` type so that we can use it in the `PyTorch` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe8f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep selected columns only\n",
    "my_cols = ord_categorical_cols + ohe_categorical_cols + numerical_cols\n",
    "my_cols = numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "X_deploy = X_full[my_cols].copy()\n",
    "y_deploy = y.copy()\n",
    "\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "X_train = tensor(X_train.to_numpy()).float()\n",
    "X_valid = tensor(X_valid.to_numpy()).float()\n",
    "y_train = tensor(y_train.values.reshape(-1,1)).float()\n",
    "y_valid = tensor(y_valid.values.reshape(-1,1)).float()\n",
    "\n",
    "X_test = tensor(X_test.to_numpy()).float()\n",
    "\n",
    "X_deploy = tensor(X_deploy.to_numpy()).float()\n",
    "y_deploy = tensor(y_deploy.values.reshape(-1,1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f3af8",
   "metadata": {},
   "source": [
    "### Creating the pipeline\n",
    "\n",
    "First we define the preproessing steps which describe how we handle the missing data. After trying different ways of dealing with missing data, the following works best:\n",
    "- Replace any missing numerical value with the `mean` value in a particular column.\n",
    "\n",
    "In this step, we also create the Regressor Neural Network model which we wrap in a `NeuralNetRegressor` object. Lastly, we bundle everything together into a ML Pipeline which we call `model_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb4cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=<class '__main__.Regressor'>,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Definition\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.first_layer = nn.Linear(36, 26)\n",
    "        self.second_layer = nn.Linear(26,52)\n",
    "        self.final_layer = nn.Linear(52,1)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        # For some weird reason .float() is needed here...\n",
    "        # Although the data has already been casted to float...\n",
    "        X = self.first_layer(x_batch.float())\n",
    "        X = F.relu(X)\n",
    "\n",
    "        X = self.second_layer(X)\n",
    "        X = F.relu(X)\n",
    "\n",
    "        return self.final_layer(X)\n",
    "\n",
    "## Declare Model\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import optim\n",
    "\n",
    "skorch_regressor = NeuralNetRegressor(module=Regressor, optimizer=optim.Adam, max_epochs=500, verbose=0)\n",
    "\n",
    "skorch_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model\n",
    "\n",
    "We are ready to tune the hyper-parameters of the model now in order to obtain the best possible version of our Deep Learning model. To do this, we make use of `GridSearch` by providing it with a list of possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        Pipeline(steps=[('imputer',\n",
       "                                                         SimpleImputer())])),\n",
       "                                       ('normalize', RobustScaler()),\n",
       "                                       ('model',\n",
       "                                        <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=<class '__main__.Regressor'>,\n",
       "))]),\n",
       "             param_grid={'model__lr': [0.02, 0.03],\n",
       "                         'model__max_epochs': [250, 500],\n",
       "                         'model__optimizer__weight_decay': [0.1, 0.2]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', numerical_transformer),\n",
    "                                 ('normalize', RobustScaler()),\n",
    "                                 ('model', skorch_regressor)])\n",
    "#model_pipeline.fit(X_train,y_train)\n",
    "\n",
    "params = {\n",
    "    \"model__lr\": [0.02, 0.03],\n",
    "    \"model__max_epochs\": [250, 500],\n",
    "    \"model__optimizer__weight_decay\": [0.1, 0.2]\n",
    "\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, params)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best model\n",
    "\n",
    "Below we print the parameters that give us the model with the best predictive ability as well as print some metrics that evaluate that ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score  : 0.7780031325060601\n",
      "Best Params : {'model__lr': 0.03, 'model__max_epochs': 500, 'model__optimizer__weight_decay': 0.2}\n",
      "\n",
      "Train MAE : 17495.9765625\n",
      "Test  MAE : 22644.798828125\n",
      "\n",
      "Train R^2 : 0.8654698012659395\n",
      "Test  R^2 : 0.7227774457623029\n"
     ]
    }
   ],
   "source": [
    "### Evaluate Model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Best Score  : {}\".format(grid.best_score_))\n",
    "print(\"Best Params : {}\".format(grid.best_params_))\n",
    "\n",
    "print(\"\\nTrain MAE : {}\".format(mean_absolute_error(y_train, grid.predict(X_train).reshape(-1))))\n",
    "print(\"Test  MAE : {}\".format(mean_absolute_error(y_valid, grid.predict(X_valid).reshape(-1))))\n",
    "\n",
    "print(\"\\nTrain R^2 : {}\".format(grid.score(X_train, y_train)))\n",
    "print(\"Test  R^2 : {}\".format(grid.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
