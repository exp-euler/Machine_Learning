{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1319dc6a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In the cell below we load the data in and split it into training and validating data. We also load the testing data.\n",
    "\n",
    "Furthermore, we inspect the data file and specify which columns correspond to numerical data and which ones correspond to categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9775f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read the data\n",
    "X_full = pd.read_csv('./input/train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('./input/test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing Price info, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We explore the dataset in order to determine the categorical variables and then determine which categorical columns are suitable for Ordinal Encoding and which ones are suitable for One Hot Encoding. Then, we encode the ordinal categorical variables according to an ordering that we judge to be the most appropriate.\\\n",
    "\\\n",
    "In this step, we also determine how to use the `SimpleImputer` in order to handle the missing data. After trying different ways of dealing with missing data, the following works best:\n",
    "- Replace any missing numerical value with the `mean` value in a particular column.\n",
    "- Replace any missing ordinal value with the `constant` value in a particular column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Select categorical columns to perform Ordinal Encoding\n",
    "ord_categorical_cols = ['Street', 'Alley', 'ExterQual', 'ExterCond',\n",
    "                       'BsmtQual', 'BsmtCond', 'BsmtExposure',\n",
    "                       'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n",
    "                       'CentralAir', 'KitchenQual', 'Functional',\n",
    "                       'FireplaceQu', 'GarageFinish', 'GarageQual',\n",
    "                       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence']\n",
    "\n",
    "# Select categorical columns to perform OneHotEncoding\n",
    "ohe_categorical_cols = [cname for cname in X_full.columns if \n",
    "                    X_full[cname].dtype == \"object\" and\n",
    "                    cname not in ord_categorical_cols]\n",
    "\n",
    "# Use SimpleImputer here just like you do encoding\n",
    "# Tensors have no columns by name apparently so one cannot use\n",
    "# ColumnTransformer in a Pipeline and work differently on different\n",
    "# columns\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_full.columns if \n",
    "                X_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Imputing missing vals in numerical cols for training data\n",
    "num_imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "num_imp.fit(X_full[numerical_cols])\n",
    "X_full[numerical_cols] = num_imp.transform(X_full[numerical_cols])\n",
    "\n",
    "# Imputing missing vals in numerical cols for testing data\n",
    "num_imp_test = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "num_imp_test.fit(X_test_full[numerical_cols])\n",
    "X_test_full[numerical_cols] = num_imp_test.transform(X_test_full[numerical_cols])\n",
    "\n",
    "# Ordinal encoding\n",
    "specified_ordering = [[None, 'Grvl', 'Pave'],\n",
    "                      ['Grvl', 'Pave'],\n",
    "                      ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'No', 'Mn', 'Av', 'Gd'],\n",
    "                      [None, 'Unf', 'Lwq', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "                      [None, 'Unf', 'Lwq', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "                      ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      ['N', 'Y'],\n",
    "                      ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'Unf', 'RFn', 'Fin'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      ['N', 'P', 'Y'],\n",
    "                      [None, 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "                      [None, 'MnWw', 'GdWo', 'MnPrv', 'GdPrv']]\n",
    "\n",
    "# Ordinal encoding of the training data\n",
    "ord_encod = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                              unknown_value=np.nan,\n",
    "                              categories=specified_ordering)\n",
    "ord_encod.fit(X_full[ord_categorical_cols])\n",
    "X_full[ord_categorical_cols]= ord_encod.transform(X_full[ord_categorical_cols])\n",
    "\n",
    "# Ordinal encoding of the testing data for the competition\n",
    "ord_encod_test = OrdinalEncoder(handle_unknown=\"use_encoded_value\",\n",
    "                              unknown_value=np.nan,\n",
    "                              categories=specified_ordering)\n",
    "ord_encod_test.fit(X_test_full[ord_categorical_cols])\n",
    "X_test_full[ord_categorical_cols]= ord_encod_test.transform(X_test_full[ord_categorical_cols])\n",
    "\n",
    "# Imputing missing vals in ordinal cols for training data\n",
    "ord_imp = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "ord_imp.fit(X_full[ord_categorical_cols])\n",
    "X_full[ord_categorical_cols] = ord_imp.transform(X_full[ord_categorical_cols])\n",
    "\n",
    "# Imputing missing vals in ordinal cols for testing data\n",
    "ord_imp_test = SimpleImputer(missing_values=np.nan, strategy='constant')\n",
    "ord_imp_test.fit(X_test_full[ord_categorical_cols])\n",
    "X_test_full[ord_categorical_cols] = ord_imp_test.transform(X_test_full[ord_categorical_cols])\n",
    "\n",
    "# For now we drop ohe columns ...\n",
    "X_full.drop(ohe_categorical_cols, axis=1, inplace=True)\n",
    "\n",
    "# Get the number of features\n",
    "input_shape = len(X_full.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validating\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n",
    "                                                                train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59dacbe",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Transforming the data to `tensor` type so that we can use it in the `PyTorch` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebe8f17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep selected columns only\n",
    "my_cols = ord_categorical_cols + numerical_cols\n",
    "\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "\n",
    "X_deploy = X_full[my_cols].copy()\n",
    "y_deploy = y.copy()\n",
    "\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "X_train = tensor(X_train.to_numpy()).float()\n",
    "X_valid = tensor(X_valid.to_numpy()).float()\n",
    "y_train = tensor(y_train.values.reshape(-1,1)).float()\n",
    "y_valid = tensor(y_valid.values.reshape(-1,1)).float()\n",
    "\n",
    "X_test = tensor(X_test.to_numpy()).float()\n",
    "\n",
    "X_deploy = tensor(X_deploy.to_numpy()).float()\n",
    "y_deploy = tensor(y_deploy.values.reshape(-1,1)).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685f3af8",
   "metadata": {},
   "source": [
    "### Creating the pipeline\n",
    "\n",
    "In this step, we create the Regressor Neural Network model which we wrap in a `NeuralNetRegressor` object. Lastly, we bundle everything together into a ML Pipeline which we call `model_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=<class '__main__.Regressor'>,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Definition\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "\n",
    "        self.first_layer = nn.Linear(input_shape, 26)\n",
    "        self.second_layer = nn.Linear(26,52)\n",
    "        self.final_layer = nn.Linear(52,1)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        # For some weird reason .float() is needed here...\n",
    "        # Although the data has already been casted to float...\n",
    "        X = self.first_layer(x_batch.float())\n",
    "        X = F.relu(X)\n",
    "\n",
    "        X = self.second_layer(X)\n",
    "        X = F.relu(X)\n",
    "\n",
    "        return self.final_layer(X)\n",
    "\n",
    "## Declare Model\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch import optim\n",
    "\n",
    "skorch_regressor = NeuralNetRegressor(module=Regressor, optimizer=optim.Adam, max_epochs=500, verbose=0)\n",
    "\n",
    "skorch_regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model\n",
    "\n",
    "We are ready to tune the hyper-parameters of the model now in order to obtain the best possible version of our Deep Learning model. To do this, we make use of `GridSearch` by providing it with a list of possible parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('normalize', RobustScaler()),\n",
       "                                       ('model',\n",
       "                                        <class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=<class '__main__.Regressor'>,\n",
       "))]),\n",
       "             param_grid={'model__lr': [0.02, 0.03],\n",
       "                         'model__max_epochs': [500, 600],\n",
       "                         'model__optimizer__weight_decay': [0.2, 0.25]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "model_pipeline = Pipeline(steps=[#('preprocessor', preprocessor),\n",
    "                                 ('normalize', RobustScaler()),\n",
    "                                 ('model', skorch_regressor)])\n",
    "#model_pipeline.fit(X_train,y_train)\n",
    "\n",
    "params = {\n",
    "    \"model__lr\": [0.02, 0.03],\n",
    "    \"model__max_epochs\": [500, 600],\n",
    "    \"model__optimizer__weight_decay\": [0.2, 0.25]\n",
    "\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(model_pipeline, params)\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best model\n",
    "\n",
    "Below we print the parameters that give us the model with the best predictive ability as well as print some metrics that evaluate that ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score  : 0.7846246222141062\n",
      "Best Params : {'model__lr': 0.02, 'model__max_epochs': 500, 'model__optimizer__weight_decay': 0.2}\n",
      "\n",
      "Train MAE : 14711.517578125\n",
      "Test  MAE : 22862.02734375\n",
      "\n",
      "Train R^2 : 0.8995725348836303\n",
      "Test  R^2 : 0.7018347580194947\n"
     ]
    }
   ],
   "source": [
    "### Evaluate Model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print(\"Best Score  : {}\".format(grid.best_score_))\n",
    "print(\"Best Params : {}\".format(grid.best_params_))\n",
    "\n",
    "print(\"\\nTrain MAE : {}\".format(mean_absolute_error(y_train, grid.predict(X_train).reshape(-1))))\n",
    "print(\"Test  MAE : {}\".format(mean_absolute_error(y_valid, grid.predict(X_valid).reshape(-1))))\n",
    "\n",
    "print(\"\\nTrain R^2 : {}\".format(grid.score(X_train, y_train)))\n",
    "print(\"Test  R^2 : {}\".format(grid.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
